================================================================================
DEEPMIND DQN TRAINING SUMMARY
================================================================================

ENVIRONMENT CONFIGURATION
--------------------------------------------------------------------------------
Grid Size: 5x5
Total States: 25
Start State: (0, 0)
Goal State: (4, 4)
Holes: []
Number of Holes: 0

NETWORK ARCHITECTURE
--------------------------------------------------------------------------------
Algorithm: Deep Q-Network (DQN) - DeepMind Architecture
Input Size: 25 (one-hot encoded state)
Hidden Layers: [128, 128]
Output Size: 4 (Q-values for each action)
Activation: ReLU (hidden), Linear (output)

HYPERPARAMETERS
--------------------------------------------------------------------------------
Learning Rate (α): 0.0005
Discount Factor (γ): 0.99
Initial Epsilon (ε): 1.0
Epsilon Decay: 0.995
Min Epsilon: 0.01
Final Epsilon: 0.0110
Batch Size: 32
Memory Size: 10000
Target Update Frequency: 1000 steps

TRAINING STATISTICS
--------------------------------------------------------------------------------
Total Episodes: 1000
Total Steps: 35074
Average Steps per Episode: 35.07
Replay Memory Size: 10000

PERFORMANCE METRICS
--------------------------------------------------------------------------------
Overall Average Reward: 0.9720
Overall Success Rate: 97.20%

First 100 Episodes:
  Average Reward: 0.8700
  Success Rate: 87.00%

Last 100 Episodes:
  Average Reward: 1.0000
  Success Rate: 100.00%

Improvement: +13.00% success rate

Best Episode: 1 (Reward: 1.0000)
Worst Episode: 0 (Reward: 0.0000)

Average Training Loss: 0.215707
Final Loss (last 100): 0.207561

================================================================================
Report generated: 2025-10-27 11:21:56
================================================================================
