================================================================================
DEEPMIND DQN TRAINING SUMMARY
================================================================================

ENVIRONMENT CONFIGURATION
--------------------------------------------------------------------------------
Grid Size: 5x5
Total States: 25
Start State: (0, 0)
Goal State: (4, 4)
Holes: []
Number of Holes: 0

NETWORK ARCHITECTURE
--------------------------------------------------------------------------------
Algorithm: Deep Q-Network (DQN) - DeepMind Architecture
Input Size: 25 (one-hot encoded state)
Hidden Layers: [128, 128]
Output Size: 4 (Q-values for each action)
Activation: ReLU (hidden), Linear (output)

HYPERPARAMETERS
--------------------------------------------------------------------------------
Learning Rate (α): 0.0005
Discount Factor (γ): 0.99
Initial Epsilon (ε): 1.0
Epsilon Decay: 0.995
Min Epsilon: 0.01
Final Epsilon: 0.0110
Batch Size: 32
Memory Size: 10000
Target Update Frequency: 1000 steps

TRAINING STATISTICS
--------------------------------------------------------------------------------
Total Episodes: 1000
Total Steps: 35192
Average Steps per Episode: 35.19
Replay Memory Size: 10000

PERFORMANCE METRICS
--------------------------------------------------------------------------------
Overall Average Reward: 0.9470
Overall Success Rate: 94.70%

First 100 Episodes:
  Average Reward: 0.8800
  Success Rate: 88.00%

Last 100 Episodes:
  Average Reward: 0.6600
  Success Rate: 66.00%

Improvement: -22.00% success rate

Best Episode: 0 (Reward: 1.0000)
Worst Episode: 5 (Reward: 0.0000)

Average Training Loss: 0.411619
Final Loss (last 100): 0.232618

================================================================================
Report generated: 2025-10-27 11:38:28
================================================================================
